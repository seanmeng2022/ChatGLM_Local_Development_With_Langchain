{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31f5d4a7-4b16-492a-81a4-8018fd960ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers --quiet \n",
    "!pip install transformers[sentencepiece] --quiet\n",
    "!pip install langchain --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be065695-b3ce-4222-923e-e1ed5841452c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e37b61e5f084701abdd8802d6957286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(model=\"lmsys/vicuna-13b-v1.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e44a16-e2ce-4e24-9078-c4fcf408fe5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b2d098-8011-46f3-b39b-ac51c4727bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"1+1 equals to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e1cecb0-e9e9-492a-8d26-ce683d0fba24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '1+1 equals to 2.\\n\\nIn the case of the 2D Is'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c55d9a-4786-4ca1-b549-94147e3b3822",
   "metadata": {},
   "source": [
    "## 提示词模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36fee0d3-4150-4aee-8721-535c3f375f65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '\\nI want you to act as a naming consultant for new companies.\\nWhat is a good name for a company that makes colorful socks?\\n\\n1. Socktopus\\n2. Rainbow Socks Co.\\n3. Kaleidoscope Socks\\n4. Sock Frenzy\\n5. Sole Society\\n6. Sock Hop\\n7. Sock Swap\\n8. Sock Studio\\n9. Sock Symphony\\n10. Sock Spectrum\\n11. Sock Serenade\\n12. Sock Sensation\\n13. Sock Sensibility'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    " \n",
    "template = \"\"\"\n",
    "I want you to act as a naming consultant for new companies.\n",
    "What is a good name for a company that makes {product}?\n",
    "\"\"\"\n",
    " \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "prompt = prompt.format(product=\"colorful socks\")\n",
    "generator(prompt, max_new_tokens = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399592c-624b-472e-896e-7e3a861a1666",
   "metadata": {},
   "source": [
    "## 将少量提示词示例传递给提示模板，实现few shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46af0da8-2589-41d4-af32-26927db3626c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input  \n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "  \n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "  Word: big\n",
      "Antonym:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Give the antonym of every input  \\nWord: happy\\nAntonym: sad\\n\\n  \\nWord: tall\\nAntonym: short\\n\\n  Word: big\\nAntonym: small\\n\\n  \\nWord: fast\\nAntonym: slow\\n\\n  \\nWord: hot\\nAntonym: cold\\n\\n  \\nWord: light\\nAntonym: dark\\n\\n  \\nWord: easy\\nAntonym: difficult\\n\\n  \\nWord: young\\nAntonym: old\\n\\n  \\nWord: good\\nAntonym: bad\\n\\n  \\nWord: true\\nAntonym: false\\n\\n  \\nWord: new\\nAntonym: old\\n\\n  '}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    " \n",
    "# 首先，创建few shot示例\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "]\n",
    " \n",
    "# 接下来，我们指定模板来格式化我们提供的示例,为此使用 `PromptTemplate` 类\n",
    "example_formatter_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\\n\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    " \n",
    "# 最后，我们创建 `FewShotPromptTemplate` 对象\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    # 这些是我们要插入到提示中的示例\n",
    "    examples=examples,\n",
    "    # 当我们将示例插入提示时，这是我们想要格式化示例的方式\n",
    "    example_prompt=example_prompt,\n",
    "    # 前缀是提示中示例之前的一些文本\n",
    "    # 通常，这由指令组成。\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    # 后缀是提示中示例后面的一些文本\n",
    "    # 通常，这是用户输入的位置\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    # 输入变量是整体提示所期望的变量\n",
    "    input_variables=[\"input\"],\n",
    "    # example_separator 是我们用来将前缀、示例和后缀连接在一起的字符串\n",
    "    example_separator=\"  \",\n",
    ")\n",
    " \n",
    "# 我们现在可以使用“format”方法生成提示\n",
    "print(few_shot_prompt.format(input=\"big\"))\n",
    "prompt = few_shot_prompt.format(input=\"big\")\n",
    "generator(prompt, max_new_tokens = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
